TrainerArgs.train_batch_size = 32
TrainerArgs.eval_batch_size = 32
TrainerArgs.log_interval = 50
TrainerArgs.eval_interval = 200
TrainerArgs.profile = False
TrainerArgs.profile_step_start = 50
TrainerArgs.profile_step_end = 80
TrainerArgs.max_train_iters = 20000
TrainerArgs.max_eval_iters = 200

TrainerArgs.top_k_for_generation = 10 # beam search width
TrainerArgs.eval_metrics = ('NDCG@10', 'Recall@10') # evaluation metrics

DatasetArgs.dataset_name = 'amzn_beauty'
DatasetArgs.max_history_length = 120 # 120 is the max sequence length for the training. We need to Downsample the original dataset to 120.
DatasetArgs.dataset_type_str = "disk_sequence_dataset"
DatasetArgs.sequence_features_training_data_path = "./tmp_data/amzn/beauty/training/22363.parquet" # training/testing
DatasetArgs.sequence_features_testing_data_path = "./tmp_data/amzn/beauty/training/22363.parquet"
DatasetArgs.item_to_sid_mapping_path = "./tmp_data/amzn/beauty/item-sid-mapping.pt"
DatasetArgs.shuffle = True
DatasetArgs.num_hierarchies = 4
DatasetArgs.codebook_sizes = [256, 256, 256, 256] # embedding vocab size for each hierarchy, the last one is used for de-duplication

NetworkArgs.num_layers = 4
NetworkArgs.num_attention_heads = 4
NetworkArgs.hidden_size = 512 # embedding dim
# per head dim
NetworkArgs.kv_channels = 128

OptimizerArgs.optimizer_str = 'adam'
OptimizerArgs.learning_rate = 1e-3

